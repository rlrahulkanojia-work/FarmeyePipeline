{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sort import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectTracker:\n",
    "\n",
    "    def __init__(self, model=\"yolov5\"):\n",
    "        # Model\n",
    "        self.model_path = \"../models/bags/farm_pipeline_0.1.pt\"\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'custom', path=self.model_path)\n",
    "        self.names = self.model.names\n",
    "        self.model.conf=0.1\n",
    "\n",
    "        #.... Initialize SORT .... \n",
    "        self.sort_max_age = 5 \n",
    "        self.sort_min_hits = 2\n",
    "        self.sort_iou_thresh = 0.2\n",
    "        self.sort_tracker = Sort(max_age=self.sort_max_age,\n",
    "                            min_hits=self.sort_min_hits,\n",
    "                            iou_threshold=self.sort_iou_thresh) \n",
    "        self.track_color_id = 0\n",
    "\n",
    "        ## Video Params\n",
    "        self.video_path = '../videos/demo.mp4'\n",
    "        self.cap = cv2.VideoCapture(self.video_path)\n",
    "        self.frame_count = 0\n",
    "        self.palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n",
    "        if not self.cap.isOpened:\n",
    "            print(\"Invalid Video.. Existing\")\n",
    "            exit(1)\n",
    "\n",
    "        self.view_img = True\n",
    "        self.color_box =False \n",
    "\n",
    "    def compute_color_for_labels(self, label):\n",
    "        color = [int(int(p * (label ** 2 - label + 1)) % 255) for p in self.palette]\n",
    "        return tuple(color)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def bbox_rel(*xyxy):\n",
    "        \"\"\"\" Calculates the relative bounding box from absolute pixel values. \"\"\"\n",
    "\n",
    "        bbox_left = min([xyxy[0].item(), xyxy[2].item()])\n",
    "        bbox_top = min([xyxy[1].item(), xyxy[3].item()])\n",
    "        bbox_w = abs(xyxy[0].item() - xyxy[2].item())\n",
    "        bbox_h = abs(xyxy[1].item() - xyxy[3].item())\n",
    "        x_c = (bbox_left + bbox_w / 2)\n",
    "        y_c = (bbox_top + bbox_h / 2)\n",
    "        w = bbox_w\n",
    "        h = bbox_h\n",
    "        return x_c, y_c, w, h\n",
    "\n",
    "\n",
    "    \"\"\"Function to Draw Bounding boxes\"\"\"\n",
    "    def draw_boxes(self, img, bbox, identities=None, categories=None, \n",
    "                    names=None, color_box=None,offset=(0, 0)):\n",
    "        for i, box in enumerate(bbox):\n",
    "            x1, y1, x2, y2 = [int(i) for i in box]\n",
    "            x1 += offset[0]\n",
    "            x2 += offset[0]\n",
    "            y1 += offset[1]\n",
    "            y2 += offset[1]\n",
    "            cat = int(categories[i]) if categories is not None else 0\n",
    "            id = int(identities[i]) if identities is not None else 0\n",
    "            data = (int((box[0]+box[2])/2),(int((box[1]+box[3])/2)))\n",
    "            label = str(id)\n",
    "\n",
    "            if color_box:\n",
    "                color = self.compute_color_for_labels(id)\n",
    "                (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2),color, 2)\n",
    "                cv2.rectangle(img, (x1, y1 - 20), (x1 + w, y1), (255,191,0), -1)\n",
    "                cv2.putText(img, label, (x1, y1 - 5),cv2.FONT_HERSHEY_SIMPLEX, 0.6, \n",
    "                [255, 255, 255], 1)\n",
    "                cv2.circle(img, data, 3, color,-1)\n",
    "            else:\n",
    "                (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2),(255,191,0), 2)\n",
    "                cv2.rectangle(img, (x1, y1 - 20), (x1 + w, y1), (255,191,0), -1)\n",
    "                cv2.putText(img, label, (x1, y1 - 5),cv2.FONT_HERSHEY_SIMPLEX, 0.6, \n",
    "                [255, 255, 255], 1)\n",
    "                cv2.circle(img, data, 3, (255,191,0),-1)\n",
    "        return img\n",
    "\n",
    "\n",
    "    def visualize(self, frame, tracked_dets, tracks):\n",
    "\n",
    "        #loop over tracks\n",
    "        for track in tracks:\n",
    "            if self.color_box:\n",
    "                color = self.compute_color_for_labels(track_color_id)\n",
    "                [cv2.line(frame, (int(track.centroidarr[i][0]),int(track.centroidarr[i][1])), \n",
    "                        (int(track.centroidarr[i+1][0]),int(track.centroidarr[i+1][1])),\n",
    "                        color, thickness=3) for i,_ in  enumerate(track.centroidarr) \n",
    "                        if i < len(track.centroidarr)-1 ] \n",
    "                track_color_id = track_color_id+1\n",
    "            else:\n",
    "                [cv2.line(frame, (int(track.centroidarr[i][0]),int(track.centroidarr[i][1])), \n",
    "                        (int(track.centroidarr[i+1][0]),int(track.centroidarr[i+1][1])),\n",
    "                        (124, 252, 0), thickness=3) for i,_ in  enumerate(track.centroidarr) \n",
    "                        if i < len(track.centroidarr)-1 ] \n",
    "\n",
    "        # draw boxes for visualization\n",
    "        if len(tracked_dets)>0:\n",
    "            bbox_xyxy = tracked_dets[:,:4]\n",
    "            identities = tracked_dets[:, 8]\n",
    "            categories = tracked_dets[:, 4]\n",
    "            self.draw_boxes(frame,\n",
    "                            bbox_xyxy,\n",
    "                            identities,\n",
    "                            categories,\n",
    "                            self.names,\n",
    "                            self.color_box)\n",
    "\n",
    "        return frame\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        frame_count = 0\n",
    "\n",
    "        while True:\n",
    "            ret, frame = self.cap.read()\n",
    "            \n",
    "            # Break the loop if there are no more frames\n",
    "            if not ret:\n",
    "                print(\"End of video or error occurred.\")\n",
    "                break\n",
    "            \n",
    "            # Increment the frame count\n",
    "            frame_count += 1\n",
    "\n",
    "            preds = self.model(frame)\n",
    "            dets_to_sort = np.empty((0,6))\n",
    "            for pred in preds.xyxy:\n",
    "                for x1,y1,x2,y2,conf,detclass in pred.cpu().detach().numpy():\n",
    "                    dets_to_sort = np.vstack(\n",
    "                        (\n",
    "                            dets_to_sort, \n",
    "                            np.array([x1, y1, x2, y2, conf, detclass])\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            # Run SORT\n",
    "            tracked_dets = self.sort_tracker.update(dets_to_sort)\n",
    "            tracks = self.sort_tracker.getTrackers()\n",
    "            frame = self.visualize(frame, tracked_dets, tracks)\n",
    "        \n",
    "            if self.view_img:\n",
    "                cv2.imshow(str(\"sa\"), frame)\n",
    "                cv2.waitKey(1) \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        # Release the video capture object and close all OpenCV windows\n",
    "        self.cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objecttracker = ObjectTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objecttracker.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Open the video file\n",
    "video_path = '../videos/demo.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Initialize frame count\n",
    "frame_count = 0\n",
    "\n",
    "while cap.isOpened() and frame_count < 50:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # Break the loop if there are no frames left to read\n",
    "\n",
    "    # Process the frame (for example, display it\n",
    "\n",
    "    # Increment the frame count\n",
    "    frame_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Object detector and Tracker \"\"\"\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sort import *\n",
    "\n",
    "\n",
    "class ZoneDetector:\n",
    "\n",
    "    def __init__(self, model=\"yolov5\"):\n",
    "        # Model\n",
    "        self.model_path = \"../models/segment/yolov5s_50_03.pt\"\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'custom', path=self.model_path)\n",
    "        self.names = self.model.names\n",
    "        self.model.conf=0.1\n",
    "        # self.model.to(\"mps\")\n",
    "        self.stride = self.model.stride\n",
    "        self.names = self.model.names\n",
    "\n",
    "        self.im0 = None\n",
    "\n",
    "    def xywh2xyxy(self, x):\n",
    "        \"\"\"Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right.\"\"\"\n",
    "        y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "        y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
    "        y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
    "        y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
    "        y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
    "        return y\n",
    "\n",
    "    def box_iou(self, box1, box2, eps=1e-7):\n",
    "        # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
    "        \"\"\"\n",
    "        Return intersection-over-union (Jaccard index) of boxes.\n",
    "\n",
    "        Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "\n",
    "        Arguments:\n",
    "            box1 (Tensor[N, 4])\n",
    "            box2 (Tensor[M, 4])\n",
    "\n",
    "        Returns:\n",
    "            iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "                IoU values for every element in boxes1 and boxes2\n",
    "        \"\"\"\n",
    "        # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "        (a1, a2), (b1, b2) = box1.unsqueeze(1).chunk(2, 2), box2.unsqueeze(0).chunk(2, 2)\n",
    "        inter = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n",
    "\n",
    "        # IoU = inter / (area1 + area2 - inter)\n",
    "        return inter / ((a2 - a1).prod(2) + (b2 - b1).prod(2) - inter + eps)\n",
    "\n",
    "    def non_max_suppression(\n",
    "        self,\n",
    "        prediction,\n",
    "        conf_thres=0.25,\n",
    "        iou_thres=0.45,\n",
    "        classes=None,\n",
    "        agnostic=False,\n",
    "        multi_label=False,\n",
    "        labels=(),\n",
    "        max_det=300,\n",
    "        nm=0,  # number of masks\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Non-Maximum Suppression (NMS) on inference results to reject overlapping detections.\n",
    "\n",
    "        Returns:\n",
    "            list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
    "        \"\"\"\n",
    "        # Checks\n",
    "        assert 0 <= conf_thres <= 1, f\"Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0\"\n",
    "        assert 0 <= iou_thres <= 1, f\"Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0\"\n",
    "        if isinstance(prediction, (list, tuple)):  # YOLOv5 model in validation model, output = (inference_out, loss_out)\n",
    "            prediction = prediction[0]  # select only inference output\n",
    "\n",
    "        device = prediction.device\n",
    "        mps = \"mps\" in device.type  # Apple MPS\n",
    "        if mps:  # MPS not fully supported yet, convert tensors to CPU before NMS\n",
    "            prediction = prediction.cpu()\n",
    "        bs = prediction.shape[0]  # batch size\n",
    "        nc = prediction.shape[2] - nm - 5  # number of classes\n",
    "        xc = prediction[..., 4] > conf_thres  # candidates\n",
    "\n",
    "        # Settings\n",
    "        # min_wh = 2  # (pixels) minimum box width and height\n",
    "        max_wh = 7680  # (pixels) maximum box width and height\n",
    "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "        time_limit = 0.5 + 0.05 * bs  # seconds to quit after\n",
    "        redundant = True  # require redundant detections\n",
    "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "        merge = False  # use merge-NMS\n",
    "\n",
    "        t = time.time()\n",
    "        mi = 5 + nc  # mask start index\n",
    "        output = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\n",
    "        for xi, x in enumerate(prediction):  # image index, image inference\n",
    "            # Apply constraints\n",
    "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "            x = x[xc[xi]]  # confidence\n",
    "\n",
    "            # Cat apriori labels if autolabelling\n",
    "            if labels and len(labels[xi]):\n",
    "                lb = labels[xi]\n",
    "                v = torch.zeros((len(lb), nc + nm + 5), device=x.device)\n",
    "                v[:, :4] = lb[:, 1:5]  # box\n",
    "                v[:, 4] = 1.0  # conf\n",
    "                v[range(len(lb)), lb[:, 0].long() + 5] = 1.0  # cls\n",
    "                x = torch.cat((x, v), 0)\n",
    "\n",
    "            # If none remain process next image\n",
    "            if not x.shape[0]:\n",
    "                continue\n",
    "\n",
    "            # Compute conf\n",
    "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "            # Box/Mask\n",
    "            box = self.xywh2xyxy(x[:, :4])  # center_x, center_y, width, height) to (x1, y1, x2, y2)\n",
    "            mask = x[:, mi:]  # zero columns if no masks\n",
    "\n",
    "            # Detections matrix nx6 (xyxy, conf, cls)\n",
    "            if multi_label:\n",
    "                i, j = (x[:, 5:mi] > conf_thres).nonzero(as_tuple=False).T\n",
    "                x = torch.cat((box[i], x[i, 5 + j, None], j[:, None].float(), mask[i]), 1)\n",
    "            else:  # best class only\n",
    "                conf, j = x[:, 5:mi].max(1, keepdim=True)\n",
    "                x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "            # Filter by class\n",
    "            if classes is not None:\n",
    "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
    "\n",
    "            # Apply finite constraint\n",
    "            # if not torch.isfinite(x).all():\n",
    "            #     x = x[torch.isfinite(x).all(1)]\n",
    "\n",
    "            # Check shape\n",
    "            n = x.shape[0]  # number of boxes\n",
    "            if not n:  # no boxes\n",
    "                continue\n",
    "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence and remove excess boxes\n",
    "\n",
    "            # Batched NMS\n",
    "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "            i = i[:max_det]  # limit detections\n",
    "            if merge and (1 < n < 3e3):  # Merge NMS (boxes merged using weighted mean)\n",
    "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
    "                iou = self.box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
    "                weights = iou * scores[None]  # box weights\n",
    "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
    "                if redundant:\n",
    "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
    "\n",
    "            output[xi] = x[i]\n",
    "            if mps:\n",
    "                output[xi] = output[xi].to(device)\n",
    "            if (time.time() - t) > time_limit:\n",
    "                print(f\"WARNING ‚ö†Ô∏è NMS time limit {time_limit:.3f}s exceeded\")\n",
    "                break  # time limit exceeded\n",
    "\n",
    "        return output\n",
    "\n",
    "    def crop_mask(self, masks, boxes):\n",
    "        \"\"\"\n",
    "        \"Crop\" predicted masks by zeroing out everything not in the predicted bbox. Vectorized by Chong (thanks Chong).\n",
    "\n",
    "        Args:\n",
    "            - masks should be a size [n, h, w] tensor of masks\n",
    "            - boxes should be a size [n, 4] tensor of bbox coords in relative point form\n",
    "        \"\"\"\n",
    "        n, h, w = masks.shape\n",
    "        x1, y1, x2, y2 = torch.chunk(boxes[:, :, None], 4, 1)  # x1 shape(1,1,n)\n",
    "        r = torch.arange(w, device=masks.device, dtype=x1.dtype)[None, None, :]  # rows shape(1,w,1)\n",
    "        c = torch.arange(h, device=masks.device, dtype=x1.dtype)[None, :, None]  # cols shape(h,1,1)\n",
    "\n",
    "        return masks * ((r >= x1) * (r < x2) * (c >= y1) * (c < y2))\n",
    "\n",
    "    def process_mask(self, protos, masks_in, bboxes, shape, upsample=False):\n",
    "        \"\"\"\n",
    "        Crop before upsample.\n",
    "        proto_out: [mask_dim, mask_h, mask_w]\n",
    "        out_masks: [n, mask_dim], n is number of masks after nms\n",
    "        bboxes: [n, 4], n is number of masks after nms\n",
    "        shape:input_image_size, (h, w).\n",
    "\n",
    "        return: h, w, n\n",
    "        \"\"\"\n",
    "        c, mh, mw = protos.shape  # CHW\n",
    "        ih, iw = shape\n",
    "        masks = (masks_in @ protos.float().view(c, -1)).sigmoid().view(-1, mh, mw)  # CHW\n",
    "\n",
    "        downsampled_bboxes = bboxes.clone()\n",
    "        downsampled_bboxes[:, 0] *= mw / iw\n",
    "        downsampled_bboxes[:, 2] *= mw / iw\n",
    "        downsampled_bboxes[:, 3] *= mh / ih\n",
    "        downsampled_bboxes[:, 1] *= mh / ih\n",
    "\n",
    "        masks = self.crop_mask(masks, downsampled_bboxes)  # CHW\n",
    "        if upsample:\n",
    "            masks = F.interpolate(masks[None], shape, mode=\"bilinear\", align_corners=False)[0]  # CHW\n",
    "        return masks.gt_(0.5)\n",
    "\n",
    "    def clip_boxes(self, boxes, shape):\n",
    "        \"\"\"Clips bounding box coordinates (xyxy) to fit within the specified image shape (height, width).\"\"\"\n",
    "        if isinstance(boxes, torch.Tensor):  # faster individually\n",
    "            boxes[..., 0].clamp_(0, shape[1])  # x1\n",
    "            boxes[..., 1].clamp_(0, shape[0])  # y1\n",
    "            boxes[..., 2].clamp_(0, shape[1])  # x2\n",
    "            boxes[..., 3].clamp_(0, shape[0])  # y2\n",
    "        else:  # np.array (faster grouped)\n",
    "            boxes[..., [0, 2]] = boxes[..., [0, 2]].clip(0, shape[1])  # x1, x2\n",
    "            boxes[..., [1, 3]] = boxes[..., [1, 3]].clip(0, shape[0])  # y1, y2\n",
    "\n",
    "    def scale_boxes(self, img1_shape, boxes, img0_shape, ratio_pad=None):\n",
    "        \"\"\"Rescales (xyxy) bounding boxes from img1_shape to img0_shape, optionally using provided `ratio_pad`.\"\"\"\n",
    "        if ratio_pad is None:  # calculate from img0_shape\n",
    "            gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n",
    "            pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n",
    "        else:\n",
    "            gain = ratio_pad[0][0]\n",
    "            pad = ratio_pad[1]\n",
    "\n",
    "        boxes[..., [0, 2]] -= pad[0]  # x padding\n",
    "        boxes[..., [1, 3]] -= pad[1]  # y padding\n",
    "        boxes[..., :4] /= gain\n",
    "        self.clip_boxes(boxes, img0_shape)\n",
    "        return boxes\n",
    "\n",
    "    def preprocess(self, im0):\n",
    "        self.im0 = im0\n",
    "        im = cv2.resize(self.im0, (640, 384))\n",
    "        im = np.transpose(im, (2, 0, 1))\n",
    "\n",
    "        im = torch.from_numpy(im).to(self.model.device)\n",
    "        im = im.half() if self.model.fp16 else im.float()  # uint8 to fp16/32\n",
    "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "        if len(im.shape) == 3:\n",
    "            im = im[None]  # expand for batch dim\n",
    "\n",
    "        return im\n",
    "    \n",
    "    def inference(self, image):\n",
    "        im = self.preprocess(image)\n",
    "        pred, proto = self.model(im)[:2]\n",
    "        pred = self.non_max_suppression(pred,\n",
    "                           conf_thres=0.1,\n",
    "                           iou_thres=0.45,\n",
    "                           classes=None, \n",
    "                           agnostic=False,\n",
    "                           max_det=1000,\n",
    "                           nm=32)\n",
    "        \n",
    "        # Process predictions\n",
    "        for i, det in enumerate(pred):  # per image\n",
    "            if len(det):\n",
    "                masks = self.process_mask(proto[i], det[:, 6:], det[:, :4], im.shape[2:], upsample=True)  # HWC\n",
    "                det[:, :4] = self.scale_boxes(im.shape[2:], det[:, :4], self.im0.shape).round()  # rescale boxes to im0 size\n",
    "        \n",
    "        masks = np.transpose(masks.numpy(), (1, 2, 0))\n",
    "        binary_mask = (masks > 0).max(axis=-1).astype(np.uint8)  # Convert to 2D binary mask\n",
    "        return binary_mask\n",
    "\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    zone_detector = ZoneDetector()\n",
    "    img = cv2.imread(\"test.jpg\")\n",
    "    binary_mask = zone_detector.inference(img)\n",
    "    print(\"Mask Shape \", binary_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/rahul/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 üöÄ 2024-11-3 Python-3.9.20 torch-2.5.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 165 layers, 7398422 parameters, 0 gradients, 25.7 GFLOPs\n",
      "WARNING ‚ö†Ô∏è YOLOv5 SegmentationModel is not yet AutoShape compatible. You will not be able to run inference with this model.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x3341e5be0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAFbCAYAAAD7pqVKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABApklEQVR4nO3de1xUZf4H8M/MAANyGcC7giIql1TETAVt8w5hmpcMMdvNNC9Zuf0wXU3LSybuGnbRLcrILptJeMlUUNIodUVL0yx1JBXNa14YZxTkMjPP7w/XsXFABWbmnIHP+/Wa1zZnnjnne54F/XjO8zxHIYQQICIiIpIhpdQFEBEREVWGQYWIiIhki0GFiIiIZItBhYiIiGSLQYWIiIhki0GFiIiIZItBhYiIiGSLQYWIiIhki0GFiIiIZEvSoKLVatG/f394e3ujSZMmmDZtGsrKyqQsiYiIiGTETaoD63Q69OnTB23btsWaNWtw5swZJCcno7i4GEuXLpWqLCIiIpIRyYJKWloaDAYD1q5di8DAQACA0WjEpEmT8PLLL6NZs2ZSlUZEREQyIdmtn+zsbPTr188SUgAgMTERZrMZOTk5UpVFREREMiLZFRWtVosxY8ZYbfP390fTpk2h1Wor/V5paSlKS0st781mMwoLC1G/fn0oFAqH1UtERET2I4TA1atX0axZMyiVlV83kXSMir+/v832gIAAFBYWVvq9lJQUzJ0714GVERERkbOcOnUKQUFBlX4uWVCprhkzZiA5OdnyXq/Xo0WLFngQA+AGdwkrIyIiontlRDl2IAu+vr53bCdZUAkICIBer7fZrtPprMat3E6tVkOtVttsd4M73BQMKkRERC5B3Pifuw3bkGwwbUREhM1YFL1ej3PnziEiIkKiqoiIiEhOJAsqCQkJ2LJlC65cuWLZlpmZCaVSibi4OKnKIiIiIhmRLKhMnDgRvr6+GDJkCHJycrB8+XJMnToVEydO5BoqREREBEDCoBIQEICtW7fCzc0NQ4YMwfTp0/HMM89g8eLFUpVEREREMiPprJ/IyEhs2bJFyhKIiIhIxvj0ZCIiIpItBhUiIiKSLQYVIiIiki0GFSIiIpItBhUiIiKSLQYVIiIiki0GFSIiIpItBhUiIiKSLQYVIiIiki0GFSIiIpItBhUiIiKSLQYVIiIiki0GFSIiIpItBhUiIiKSLQYVIiIiki0GFSIiIpItBhUiIiKSLQYVIiIiki0GFSIiIpItBhUiIiKSLQYVIiIiki0GFSIiIpItBhUiIiKSLQYVIiIiki0GFSIiIpItBhUiIiKSLYcElY8//hgKhcLmNX36dKt26enpCAsLg6enJzp27IgNGzY4ohwiIiJyUW6O3PmmTZug0Wgs75s3b27575UrV2LcuHGYOXMm+vTpg4yMDAwdOhTbt29HTEyMI8siIiIiF+HQoNK5c2c0aNCgws9mz56NpKQkvPbaawCA3r1748CBA5g3bx6ysrIcWRYRERG5CEnGqBw/fhz5+flITEy02p6UlIStW7eitLRUirKIiIhIZhwaVNq1aweVSoXQ0FCkpKTAZDIBALRaLQAgIiLCqn1kZCTKyspQUFBQ6T5LS0thMBisXkRERFQ7OeTWT9OmTTF37lx069YNCoUCX3/9NWbNmoUzZ85g6dKl0Ol0AAB/f3+r7wUEBAAACgsLK913SkoK5s6d64iyiYiISGYcElTi4+MRHx9veR8XFwcvLy+8+eabmDlzZo32PWPGDCQnJ1veGwwGBAcH12ifREREJE9OG6OSmJgIk8mE/fv3W66c6PV6qzY3r7QEBgZWuh+1Wg0/Pz+rFxEREdVOkgymvTk25eZYlZu0Wi08PDwQGhoqRVlEREQkM04LKitXroRKpUKnTp0QGhqKsLAwZGZmWrXJyMhA37594eHh4ayyiIiISMYcNkalT58+6NChAwDg66+/xgcffIC///3vaNKkCQBgzpw5GDVqFFq3bo3evXsjIyMDu3fvxrZt2xxREhEREbkghwSViIgIpKen4/Tp0zCbzQgLC8Nbb72FF154wdJm5MiRKC4uxsKFC7Fw4UKEh4dj7dq1iI2NdURJRERE5IIUQgghdRE1YTAYoNFo0AuD4aZwl7ocIiIiugdGUY7vsA56vf6OE2P49GQiIiKSLQYVIiIiki0GFSIiIpItBhUiIiKSLQYVIiIiki0GFSIiIpItBhUiIiKSLQYVIiIiki0GFSIiIpItBhUiIiKSLQYVIiIiki0GFSIiIpItBhUiIiKSLQYVIiIiki03qQsgIudwCw3B33M2oonbVacc75TRH/8ePBgQovo7EQKmw7/VuBaFWg1l65Y13o+Ny1dg+uOC/fdLd+TWMhjC2+ue2yuKrsN48pQDKyJHYlAhqgNEj2gMWLYVcfXKAXg65ZhRHiV45JuMGu2j2FyG7v96EQqT7WdKo0CDD3ZZgpAqIADnnoiseD/NBI48/V6NaqlI74ODceWr7nC/JhDwSZ7d90+36J+MQanmxk2AJ57djKmBx+75u/MvRWD1+30q/bzxbgPEnl9rXCM5hkKImvxzR3oGgwEajQa9MBhuCnepyyGSpd/ejsHxx9OkLsOuis1liN7xDIT5xl9efr7F2Nv5S0lqKSi/hrj/Po/m/3GHOutHSWqo7RIOXsGLASccsu/JZ7sgO7+d1Taj3gNhz/7gkOPRDUZRju+wDnq9Hn5+fpW24xUVInJJ9ZQeyH/oU6nLAAC0cvfBb70+xnfdlMh+PQoHHvKF+apzbrFRzb3T7EegmXXALDaXYXnP1tbt1g5E60WHIExm/v/rRAwqRER20svLjF5e+zE8qx+u/oV/kdmLMvo+BLtvcOox6yk98Jy/9biWiaP/DfNoga3X62HBlKcAAN7fHmZocTAGFSIiOxveaC/eG/Q4PNfz1kGNxUSh/bu/4DEfg9SVQKVQQgXg4XqlePi9DwAArbKfgarwxrCDsOWXYTqUL2GFtRODChGRnSX56jCrpxKt10tdieu72Mkbi5rsk7qMShUkfGj571n9OyD/WiPL+6K/+sBYcFKKsmoVBhUiIpIlVdtQbJyxCICP1KXck/mNfgFu5RQUfHcNzx4bAeXIck5jrwEu+EZERPKkVKKpm2uElIq0cvfBpoiNuPSRBn9M7g6FG68NVAeDChGRA/y1/zYoHmgvdRku7di8elKXYBc/dMrE3n8sxekvw3Hhue5Sl2NF4e6Bgi864vfMDjD37CR1ORVivCMicoDZDQ/hL8F/Qb09Ulfiuj7rmg6gdqyPpVIo8WvM5zh8fzH+dn0KGq7VwqTTSVbPtcQYjJu3BkqY8Te/G4O+t96vQurAYRC/n4W5qEiy2m7HKypEREROEulRDz/Ofw/T9nwvaR1lPgqM9ruAv/ldsmzr62XC+i0ZqP+NO1RtQyWszlqVg8rRo0cxceJEREdHw83NDe3bV3xpMz09HWFhYfD09ETHjh2xYYPtHHi9Xo+xY8ciMDAQvr6+GD58OM6dO1f1syAiInIhSoVZsmOrAgKgeOxSxZ8plPhPyHc4ObyJk6uqXJWDysGDB7Fx40a0adMG9913X4VtVq5ciXHjxmHEiBHIzs5GbGwshg4dil27dlm1GzFiBHJycpCWlobPP/8cR44cQUJCAoxGY/XOhoiIyFUoFNIc1t8PP3TKlOTY1VHlMSqDBg3C4MGDAQCjR4/Gnj22N2Bnz56NpKQkvPbaawCA3r1748CBA5g3bx6ysrIAAHl5edi8eTM2b96MuLg4AEB4eDgiIyOxZs0aJCYmVvukiIiI5KyH2gzzliB4jCqD8fwfUpdjo6ShGUpPT5hLSqQupepXVJTKO3/l+PHjyM/PtwkaSUlJ2Lp1K0pLSwEA2dnZ8Pf3R//+/S1twsPDER0dbQkzREREtZFKocQ3ketx4UMN3Jo3k7ocG8eS0mDqFC51GQAcMJhWq9UCACIiIqy2R0ZGoqysDAUFBZZ24eHhUNx26SsyMtKyj4qUlpbCYDBYvYiI5ObBA8Pgt49j7ujOfrz/S5g+U+Do4hinHdN84RJabR7rtOPVlN2Diu5/0638/f2ttgcEBAAACgsLLe1ub3Oz3c02FUlJSYFGo7G8goOD7VM4EZEdnf+1EYwnfpe6DHIBmyI2Yliv3U47nrmoCA22ezjteDXlctOTZ8yYAb1eb3mdOnXq7l8iIiKSqXJhwuoD90tdhmzZPajcvHKi1+uttt+80hIYGGhpd3ubm+1utqmIWq2Gn5+f1YuIiGqfkV+9IHUJTnHNXIqwCb869Zj1D1zF82e6OfWY1WX3oHJzbMrt40y0Wi08PDwQGhpqaXfkyBEIIWza3T6+hYhqRsIlG4iqLTyt4rU+aiOFSuXU44k9v2Lj/iinHrO67B5UQkNDERYWhsxM6znaGRkZ6Nu3Lzw8btwXS0hIgE6nw9atWy1t8vPzsW/fPgwYMMDeZRHVaW2n78O4Uz2kLoOIKhCgqodm3zp/JEb48wcw/5L8LwxUeR2V4uJiy/ThkydPwmAwYNWqVQCAnj17omHDhpgzZw5GjRqF1q1bo3fv3sjIyMDu3buxbds2y35iY2MRHx+PMWPGIDU1FZ6enpg5cyaioqIwbNgwO50eEQGAKC3F9s33o3hMLuopXWcQHVFdEehRhNNOPqYoLcXHm3vjH08chLvCuVd0qkIhbr/3chcnTpxAq1atKvwsNzcXvXr1AnBjCf2FCxfi999/R3h4OBYsWICBAwdatdfr9UhOTsaaNWtgNBoRFxeHJUuWoFmze59TbjAYoNFo0AuD4aaoHQ+vInKUS+NjYXa7sSTAh1PfQmc1Q4ujtFkxEa1f2nX3hlQplZ8ftO+0xfG4dKlLcbj9paV4OvX/0GjpTpvPFA+0x8XOvmjwfp79D6xU4fjCrvjtyfesNkfs+CtCnzsP08WL9j/m/xhFOb7DOuj1+juON61yUJEbBhWi6nELDYFwd8Ph5ACsjVsKtcKESI96UpdVazCo2MfFibH46dX37t6wFigov4ZD5Q2wdNgQmA9oofLXAI0aYFr2WgS7GaAtb4DNVzrgaH8fuz55WenpiaPzOmHPE4uhUXoBANotmYSgFNvQZE/3GlSqfOuHiGoH4/ETAICwCcA/0A3K9hEQ71wFAHzRNhMBKoaWmvBuo4db0yYwnjsvdSnkIlq5+6CVewmarPsUz70yGYFjf0dW+Or/feqD1u4leKTej0ja0AeGp1vDlH/MLsc1l5QgdFoeOiv/D3uT3rSEFblwuXVUiMgxzL9qIfqcgehzBt3+M0Xqclzez12/gCGmpdRlkAvqrPbArn+lISu84sfJrGz1LZQfFOPoW/Zdzbb1S7vQ+Ytku+7THhhUiMhG6Jyf0HvMOHT6MUnqUoioAhvCsnHo8SVov1eJs1O7Q+Fun/FmbV/Zh95jxiHkC2cP7a0cgwoR2RClpfDY9CMaDz/uEtMXieoitcIdqU1/wu6/v4Xlx75F2cNdYH4wGqrGjaq9T3NJCTw2/Sirxz9wjAoRVUqUl6FcyHfaIhEB9ZQeqKf0QO5HywAA7XeNQotxRpguV/7cPFfCKypERES1yK8xnwONG0hdht0wqBAREdUyiWu+q9EtIDlhUCEiIqplRvtdgPmL2rG2GIMKERERyRaDChFVTKHAifmxmBiwW+pKiKiKjpVfA6b4S12GXXDWDxFVSPdUDH59eincFT5Sl0JEVVQk3CD2HZS6DLvgFRUiqpBQQtZPVCWiyjVUGqEbHSt1GXbBoEJENlSNG+Gh53jLh6TlFtQc/Sc44InBdUBTNx+8+eq/UTjG9cMKgwoR2VDU80Jq05+kLoPqOOHthX823i91GS6rh6cSq+csgmFkDFRtWkHVoL7UJVULgwoR2Tj7SHOpSyAiO2jh5oO81DRkbVsL5SoPXH4mFkpvb6nLqhIOpiUiG2/+X5rUJRCRnW0IywbmAZHxf4XbD75otmin1CXdE15RISIr+Wld0VVdInUZROQgh3t8htXPL8JfDpTA/GC01OXcFYMKEVm4hbRAbIffUE9pn0fGE5E8hbl7Y1YDLT7/4t8w/6WT1OXcEYMKEVkU/DUIK1rlSl0GETlJI5U3Xlz+BU7Mj4WyY6TU5VSIQYWIAADK9hEYn5QldRlE5GSP1CvBkTHvIerjw7KcGcSgQkQAAGOAF14MOCF1GUQWieu2SV1CnfLPxvuR8P1RuDVtInUpVhhUiAhQKPB7vKfUVRBZuU99RuoS6pwXAk7iwjI/qcuwwqBCRCh4PQYHnn5H6jKIiGwwqBDVcQq1GoPid0OtcJe6FCKSATeVCQp3+cz8Y1AhquOOvNmRy+UTkcWOqEycnvKA1GVYVDmoHD16FBMnTkR0dDTc3NzQvn17mza9evWCQqGweWm1Wqt2er0eY8eORWBgIHx9fTF8+HCcO3eu+mdDRFWnkLoAIpITlUIJIaM/F6q8hP7BgwexceNGdOvWDWazGWazucJ2PXr0wBtvvGG1LSQkxOr9iBEjcPDgQaSlpcHT0xMzZ85EQkIC9uzZAzc3ru5PRFRXHV0cg/vcdwHgLcm6rsppYNCgQRg8eDAAYPTo0dizZ0+F7fz9/RETE1PpfvLy8rB582Zs3rwZcXFxAIDw8HBERkZizZo1SExMrGppRFRFSl9fqPzKpS6DyEZI1Fn4KDkTjapx60eptM+wluzsbPj7+6N///6WbeHh4YiOjkZWFhedInIG3eB2ONp7udRlEBFVymGDab///nt4e3vD09MTPXv2xLZt1gv3aLVahIeHQ6GwvhEWGRlpM5blz0pLS2EwGKxeREREVDs5JKj07NkTb7/9NjZt2oRPPvkExcXF6NevH/Ly8ixtdDod/P39bb4bEBCAwsLCSvedkpICjUZjeQUHBzviFIhqPVXDhpgzh1dTSH7OTemO5WErpC6DZMIhI1bnzp1r9X7gwIFo164dXnvttRrf1pkxYwaSk5Mt7w0GA8MKUTXoe7fGw/W+kboMIhul/gIt3HykLoNkwinrqHh7e+ORRx7B3r17LdsCAgKg1+tt2up0OgQGBla6L7VaDT8/P6sXEVVN4ZhYfPivxVKXQUR0V5It+BYREYEjR45ACGG1XavVIiIiQqKqiOoG42AdIj3qSV0GUcVktIYHSc8pQaWoqAgbNmxAly5dLNsSEhKg0+mwdetWy7b8/Hzs27cPAwYMcEZZRHWOQq1GwYJYbO/8sdSlEFXI2Kcz/vvUG3dvSHVGlceoFBcXW8aZnDx5EgaDAatWrQJwYxCtVqvFokWLMHToUISEhODs2bNITU3F+fPnkZmZadlPbGws4uPjMWbMGKSmploWfIuKisKwYcPsdHpE9GfHZ9+P/NHvAeD6FCRPwk2BBipvqcsgGalyULlw4QIef/xxq2033+fm5iIoKAhlZWV4+eWXcfnyZXh7e6N79+5IS0tD165drb6XkZGB5ORkjB8/HkajEXFxcViyZAlXpSVyBKUKW55cBICDFInIdVQ5EYSEhNiMK7ndpk2b7mlfGo0G6enpSE9Pr2oZRFRFF9a2RXPVj1KXQVQppacnnlv6pdRlkMzw6clEdUSHRmehUvBXnmRMqUR8vQtSV0Eywz+1iIiISLYYVIiISB4UnJdMthhUiIhIFlp9Z+ITk8kGgwoREclCQ4+rUpdAMsSgQkRERLLFoEJERESyxaBCREREssWgQkRERLLFoEJUFygUUCnuvKI0EZEcMagQ1QHHFnXDB8HfSV1GnaNvpQKUKqnLIKqS/PIieBbK5x82DCpEdYDZHXBX8C9MZ9s7ZQlUGj+pyyCqklG/PI0G7+dJXYYFgwoRERHJFoMKERERAQDKhQn6q15Sl2GFQYWoDtD8psT+0lKpy6hzlFBA93C41GW4hpgohHmel7qKOi/nujdajfxZ6jKsMKgQ1QGNlu7E4vP9pS6jzlEplHjylY1Sl+ES8sd5YJTvZanLqPOmrHha6hJsMKgQERERAKD1slNSl2CDQYWIiIhki0GFiIiIEH94IMRV+T3BmkGFiIiIoPs0GKYreqnLsMGgQkREklLWqwc3L6PUZZBMMagQEZGkLiV1xG+9Ppa6jDrtU0MD+Jwpk7qMCjGoEBER1XGztw2F+5a9UpdRIQYVIiKiOswkzIBJIXUZlWJQISIiqsNevnA/wl/4SeoyKsWgQkREVIeZhQLCKN/BzFUKKpmZmRg8eDCCgoLg7e2N6OhofPTRRxBCWLVLT09HWFgYPD090bFjR2zYsMFmX3q9HmPHjkVgYCB8fX0xfPhwnDt3rmZnQ0RERLVKlYLK4sWLUa9ePaSmpmL9+vVISEjAuHHjMG/ePEublStXYty4cRgxYgSys7MRGxuLoUOHYteuXVb7GjFiBHJycpCWlobPP/8cR44cQUJCAowyTnVERES1zS9Xmkldwh0pxO2XQ+7g0qVLaNCggdW28ePHIyMjAzqdDkqlEuHh4ejcuTNWrFhhadO9e3f4+/sjKysLAJCXl4fu3btj8+bNiIuLAwAcOXIEkZGRWLlyJRITE+/5BAwGAzQaDXphMNwU7vf8PaK65uibMfjl8XdQT+khdSl1yr+vBOPr++pLXYZsKb29oV18HwoGLZO6lDpJZyrGyDa9ISR4urpRlOM7rINer4efn1+l7ap0ReX2kAIAnTp1gsFgQFFREY4fP478/HyboJGUlIStW7ei9H8dkZ2dDX9/f/Tvf+tpruHh4YiOjraEGSKyrzZTfsQlszzXSaC6S9moAUMK3ZFbTXewY8cONG/eHL6+vti+fTsAICIiwqpNZGQkysrKUFBQgIiICGi1WoSHh0OhUNi002q1dzxeaWmpJfAAN66oENG9uWjyQD1FkV325av0gJpXMYnIwWoUVHbs2IGVK1ciNTUVAKDT6QAA/v7+Vu0CAgIAAIWFhZZ2t7e52e5mm8qkpKRg7ty5NSmbqG4ymzAz7EG77e74vM4YEPfjPbVVwYyFTX6Eu0Jlt+MTUd1Q7aBy+vRpjBgxAr1798bkyZPtWdMdzZgxA8nJyZb3BoMBwcHBTjs+kSsT5fa79dNqRh4Oz7jHxgoFwt96DkJtrtpBlAL5j6S5bMAxCTPe/nogWiFP6lKIKhSzcyJCyg9KXcYdVSuoXLlyBQkJCahfvz5Wr14NpfLGUJebV070ej2aNGliaX/zSktgYKCl3alTp2z2q9PpLG0qo1aroVarq1M2EUlFCLT9+667t7udUoWHkp6D+N9ouk9fT0WYu7d9a3MgMwTa/FMLk9SFEFUiOM0NMMv7J7TKQeX69esYOHAg9Ho98vLyoNFoLJ/dHJtycwzKTVqtFh4eHggNDbW027JlC4QQVuNUtFotOnToUO2TIaJaxmyC34pbAef/tgwD/vdnhvCph9EbtyJafVa24SWvVAWY5P2XAJHcVWnWj9FoRGJiIg4fPoxNmzahefPmVp+HhoYiLCwMmZmZVtszMjLQt29feHjcmBaZkJAAnU6HrVu3Wtrk5+dj3759GDBgQHXPhYhqOeP5P2A8dx7Gc+dh+u04Pun7Fzz562ipy6rUzCkTYOKAf6IaqdIVlUmTJmHDhg1ITU2FwWCwWsStU6dOUKvVmDNnDkaNGoXWrVujd+/eyMjIwO7du7Ft2zZL29jYWMTHx2PMmDFITU2Fp6cnZs6ciaioKAwbNsx+Z0dEtZbS0xNXP3THDx0y795YAv0PD4LvLxd424dkK/bnxxD42x+Q+zKrVQoqOTk5AIApU6bYfFZQUICQkBCMHDkSxcXFWLhwIRYuXIjw8HCsXbsWsbGxVu0zMjKQnJyM8ePHw2g0Ii4uDkuWLIGbW41nTBNRJZTe3jAX2Wd6shSUvr6W28Xmr3yxLWKtxBVV7MVzD8DjKTOMpwukLkX2RD1PqUuos67sbgy/0zulLuOuqpQKTpw4cU/txo4di7Fjx96xjUajQXp6OtLT06tSAhFVl1KF+7Zdx6+dpS7kzhRqNa7371jhZ0uWvoNw9xszgOS8houPqhTC20vqMlzC5HVfSV1CnbS/tBT1zt3zwvSS4uULojqkm+8x7HxipNUAVamdf7E7ipvd+gPT6GNCwZAPKmntGv/6nt/oF7QZ1wOtXzoqdSmy56kol7qEOmlyfhIavO8a0+YZVIjqkBjPM7g0sAR+K+7e1lEKUmJxX/fjlvcrW76BSI960hXkIK8OXIXPvhoI5Y79UpdCZMMsFHdvJBMMKkR1zJ6H3kPszCkIWXsJpkP5Tj9+ecNyrGi9/k8PR6x9IQUA/uZ3Ccsaqmvp2dmHW8tgeCr2AnDNBf1c1a4SE/wSL7nMQO8qTU8mItd1NbELfBVKaJReOPTcu/hg00f4Y3J3/DG5O9yCg5xWR9gze9Ahd4LTjkfyVZjmgRhPhhRnK4PKpabN84oKUR3R5u+HEKC69e/7IDcf7J/+LgAgfvBAnLx8a7HF0OfOw3TxosNqCZtwBFHPTkKLQQXYEJbtsONIzfuF01BsrgdzcbHUpRC5LAYVojri0pjG2LXRhGZu16229/p6CsKWX0MIbj2Hx/y/x144irm4GE1Td0JkNEd80N/wzy+WIVBlPagyUOkGH6VrDJ4FgGJzGS6Zbz1LqWf2/yH8g2KYr5+VsCoiW2fLA6QuoUoUQgjXmJ9UCYPBAI1Gg14YDDcZT1ckkgVFBQPo5PBHQAV1nXo5Fg8M/FWCYqpn1/ft0GrGn2ZTyaFfZUzVNhSNP7uI5S22S11KnTOg5zCYfjt+94YOZhTl+A7roNfr4efnV2k7XlEhqkvk+pdnBXUFv74Tf7wuQS3VxCckV82JpCbIarFG6jLqJIWpik8xlxgH0xIREZFsMagQEZFTKT09UdLQVSbH1i7RCyfBWHBS6jKqhEGFiIicytglEseHvy91GXWS23Uh31vAlWBQISIiqgMSj/dFo/9elrqMKmNQISIip1E1bIjRH34tdRl10p6jIZKsRl1TDCpEROQUik7tkPBdPkb5ut6/6l3d78Zr8MpXS11GtTCoEBGRwyk6t0PYsny8EOBaAzlri/9c6YygBTulLqNauI4KERE5lKpxI/T7JA/JgdIvMkauh0GFiIjsTunpCXNUWwDAqM+yeLtHYt/8EQEPuObVLAYVIiKyqyt/jcWlTgLHktKkLoVw4zlUnkMuwbXWo72FQYWIiOxCxHbExemleK/DUsR4qqQuh2oJBhUiIqoxVdtQTP/8EzzkCQAMKXLS89W/I7B4190byhSDChER1Vji+h3/Cyl0L3SmYow6OrzCz75ouxoapZddjvOBvhkCf73mcqvR/hmDChERkZM8f6YbcrbcD7ciBYLnVzxdOPaVKTDWExgUtxupTX+q0fHe+GowWv3g2k/2ZlAhIiJysHJhwnJDMI493Qqtfr1zcAh+7UaAOfRFBHoFdcbFju448PxSqBRVW/rMJMxQuOoI2j9hUCEiInKgbSXAnIkToN5xCOZi7T1/z3xAC/UBIGiTClF4HtnP/gst3Hzu+ftDjw5Aqzl74bo3fW5gUCEiInIAnakYD3z3HJqsV8MnZ1f1pwebTQhK2YmHVdOQNzH1nsevlJlUEOVl1T2qbHAJfSIiIgd46J2X0Oav++DzpX1m3ATP34keS6fcU9vfjddw/c3mdjmu1KoUVDIzMzF48GAEBQXB29sb0dHR+OijjyD+NJq4V69eUCgUNi+t1vpyl16vx9ixYxEYGAhfX18MHz4c586ds89ZERERSaBUlKNN7tMY0HMYmr/5g933H/TGD2i3ZBIumYru2G779ZbwXG//40uhSrd+Fi9ejJCQEKSmpqJhw4b45ptvMG7cOJw6dQqzZ8+2tOvRowfeeOMNq++GhIRYvR8xYgQOHjyItLQ0eHp6YubMmUhISMCePXvg5sY7UkRE5HrafT8OrUftg8lB+xdGI4JSduIht6nIGfcvBFUyZuXzoX0B5DuoCueqUiJYv349GjRoYHnfp08fXL58GYsXL8Yrr7wCpfLGBRp/f3/ExMRUup+8vDxs3rwZmzdvRlxcHAAgPDwckZGRWLNmDRITE6tzLkRERJIpFyaEPXfSYSHlz4Jf24l40zQcfP5dm886/ZiEphcuOqEK56jSrZ8/h5SbOnXqBIPBgKKiO1+G+rPs7Gz4+/ujf//+lm3h4eGIjo5GVlZWVUoiIiKSXLkwodM7L8CkNzjtmMH/unEbqFzcikYx+4ej2cQrMF2qPQ+BrPFg2h07dqB58+bw9fW1bPv+++/h7e0NT09P9OzZE9u2bbP6jlarRXh4OBQKhdX2yMhIm7EstystLYXBYLB6ERERSSki8zk0/1ceYHbG9ZQbhNGIoIV56PjeC5h78T48eGAY/Af/DuO5806rwRlqFFR27NiBlStX4qWXXrJs69mzJ95++21s2rQJn3zyCYqLi9GvXz/k5d1a4Ean08Hf399mfwEBASgsLLzjMVNSUqDRaCyv4ODgmpwCERFRjSmMkGaZeiEQPH8ndnb0gPfDx2vFdOTbVXvU6unTpzFixAj07t0bkydPtmyfO3euVbuBAweiXbt2eO211+xyW2fGjBlITk62vDcYDAwrREREtVS1rqhcuXIFCQkJqF+/PlavXm0ZRFsRb29vPPLII9i7d69lW0BAAPR6vU1bnU6HwMDAOx5brVbDz8/P6kVERNLKGN4HJlEL1msn2alyULl+/ToGDhwIvV6P7OxsaDSaKh80IiICR44csVp/BbgxdiUiIqLK+yMiImkpzteeWSZV9UNpOTT5irs3pGqpUlAxGo1ITEzE4cOHsWnTJjRvfvdV74qKirBhwwZ06dLFsi0hIQE6nQ5bt261bMvPz8e+ffswYMCAqpREREQyIK4Vof37zyMybRI+NdjOEK3NFp+NR4MPXPsJxXKmELdf1riD8ePHY9myZUhNTUX37t2tPuvUqRN++OEHLFq0CEOHDkVISAjOnj2L1NRUHDx4ENu3b0fXrl0t7R9++GEcOnQIqamplgXflEpllRd8MxgM0Gg06IXBcFO43/P3iIjIMZTR9+HS/RrkzE2Fj1INd4VK6pIcKqmgD3Q97jwRhGwZRTm+wzro9fo7DuOoUlAJCQnByZMnK/ysoKAARqMRzz//PH7++WdcvnwZ3t7e6N69O2bPnm0VUoAbS+gnJydjzZo1MBqNiIuLw5IlS9CsWbN7LQcAgwoRkVwp1GrkL4rGvmFv3fOD9FxNqSjHfd+OR5u/7pO6FJfjkKAiRwwqRETy9tvbMTj+eJrUZTjEsfJrmBTyF2mmJru4ew0qfHoyERE5VNi0fQjNnCh1GQ4xZOk0hhQHY1AhIiKHEuVG+B6vnX/dNN9iu9QG2Vft/MkhIiLZcGvcED//w/bhea7uH39EQ3XlmtRl1HoMKkRERNWQ/Vl3GAsqnmBC9sOgQkREDmW6eAn3/XuS1GWQi2JQISIihxJGI/xOcnl9qh4GFSIiIpItBhUiInIoVf1A9JmyU+oyyEUxqBARkUMZw4KxoPEBqcsgF8WgQkREDjXp09VSl0AujEGFiIgc6pUP/iZ1CXY3+WwXNPuei705A4MKERE5VIvPjuGSqUjqMuxq68kwiL0HpS6jTmBQISIihzKe/wPDJ7yIjcWeUpdiF5dMRSj7rfKH6JF9MagQEZHDqbN+xOszR2NTsVrqUmpsS3EQQqfnSV1GncGgQkRETuGbsQsbdNEwCS7+RveOQYWIiJzmeH9PDIqOR/t3JuG0kQ/0o7tjUCEiIqcxXdHDdPEimi/cibj3p+FCLRtkS/bHoEJERJIIfn0nDpX5Sl1GlaW8O1LqEuoUBhUiIpLM7+WBUpdQJSZhRvOVR6Uuo05hUCEiIslkDHhQ6hJI5hhUiIhIOkJIXQHJHIMKERFJypWmK5vBYOVsDCpERCQZ44nf8cCC56Uu4551n/U8TH9ckLqMOoVBhYiIpCME3K67zlUK92LXqbW2YFAhIiJJNfzvRQz5LV7qMkimGFSIiEhSpiNHceDXEOSXc/E3slWloJKVlYWePXuiYcOGUKvVCA0NRXJyMvR6vVW79evXo2PHjvD09ERYWBiWL19us6+ysjJMnToVTZo0gbe3N/r3748jR47U7GyIiMgltX1uNyaH9cGkMzFSl0IyU6WgUlhYiG7duiEtLQ2bN29GcnIyPv30Uzz++OOWNjt27MDQoUMRGxuL7OxsjBgxAmPHjsWqVaus9jV58mQsW7YMCxYswJo1a1BaWoq+ffvahB4iIqobRGkptDPbS10GyYxCiJpNYl+2bBnGjx+PM2fOoFmzZoiPj8e1a9fw3//+19LmiSeewP79+3Ho0CEAwOnTpxESEoJ3330X48ePB3AjBLVo0QKvvvoqpk2bds/HNxgM0Gg06IXBcFO41+RUiIhIYqqAAGjfboXj/T6SuhQbHXY/geCJlzjrx06MohzfYR30ej38/PwqbVfjMSr169cHcONWTmlpKXJzc62usABAUlISDh8+jBMnTgAAcnJyYDabrdoFBgYiLi4OWVlZNS2JiIhclEmng+8+TxSUy+/JykVnfBlSJFCtoGIymVBSUoKffvoJ8+bNw6OPPoqQkBAcO3YM5eXliIiIsGofGRkJANBqtZb/bdSoEQICAmza3WxTmdLSUhgMBqsXERHVHk3e3IlnfntC6jJIJqoVVFq2bAkvLy907twZTZs2xYoVKwAAOp0OAODv72/V/mYgKSwstLS7vc3NdjfbVCYlJQUajcbyCg4Ors4pEBERkQuoVlDJysrCzp07sWzZMhw+fBiDBg2CyWSyd20VmjFjBvR6veV16tQppxyXiIicx3O0CQfKSqQug2TArTpfioqKAgDExsaiS5cuiI6Oxtq1a3HfffcBgM3MnZtXWgIDbzzOOyAgoMLZPTqdztKmMmq1Gmq1ujplExGRizCePoPnj4zEtg5rpS6FJFbjwbRRUVFwd3fH0aNH0bp1a7i7u9uMM7n5/ubYlYiICPzxxx+WAPPndrePbyEiorrJ5wk9Ov2YJHUZJLEaB5Xdu3ejvLwcoaGhUKvV6N27t82aKRkZGYiMjERISAgAIC4uDkqlEqtXr7a00el0yMnJwYABA2paEhER1QKmy4VoNsmA3mPG4WNDI5SKcqlLIglU6dbPsGHD8MADDyAqKgpeXl74+eefsWjRIkRFRWHIkCEAgFdeeQW9evXCpEmTkJiYiNzcXKxYsQIZGRmW/QQFBeGZZ57B1KlToVKp0Lx5cyxYsAAajQYTJkyw6wkSEZHrMp45C48zZ5ERHYpPeg3G3HeX4SFPqasiZ6pSUOnatSsyMjKwcOFCmM1mhISEYNy4cXjppZfg4eEBAHjwwQexZs0azJo1C+np6WjRogU+/PBDm7VV3n77bfj4+GD69Om4evUqevTogS1btkCj0djv7IiIqFYwl5TAY9OPmD5jIqbP/xSPehdLXRI5SY1XppUaV6YlIqpbSgZ1xcWObjg06V2nHjd0zQS0fX63U49ZmzltZVoiIiJn8lz/A1os/AEDHhqKsO+fwg+lHLtSmzGoEBGRyxFGI0xHC9Bq5M+Y07Evn7pcizGoEBGRSzMZDDjxt2AMP9ZP6lLIARhUiIjI5ZkO/4bip30x5dz9UpdCdsagQkREtYLpaAEO9fDAhNOxmH2xHZfgryUYVIiIqNYwl5TgRNfr2NXRHX9LTUZB+TWpS6IaYlAhIqJaqfGSnfi5rIld9vXvK8Fo+5/rdtkXVQ2DChER1VrvDx9kl1tA+6+2AHYdsENFVFXVenoyERGRKzD/fBjJT0/C9X9cuWO7DyP+g0iPehV+VirKsfWH9mgLLvYmBQYVIiKq1VS5P8En985tRk2cAt39RhQMXGbZNvtiO2Rm9oTCBLRdmOfgKqkyDCpERFTnNUzLQxM/P/T//GnLNvdLxQg+uFPCqghgUCEiIgJwY+E45ff7br2XsBa6hYNpiYiISLYYVIiIiEi2GFSIiIhIthhUiIiISLYYVIiIiEi2GFSIiIhIthhUiIiISLYYVIiIiEi2GFSIiIhIthhUiIiISLYYVIiIiEi2GFSIiIhIthhUiIiISLaqFFSysrLQs2dPNGzYEGq1GqGhoUhOToZer7e0GT16NBQKhc1r06ZNVvsqKyvD1KlT0aRJE3h7e6N///44cuSIfc6KiIgcTuntjetDulpe5gejpS6JaiG3qjQuLCxEt27dMHnyZNSvXx+//vor5syZg19//RU5OTmWdqGhofj888+tvhsZGWn1fvLkyVi5ciUWL16M5s2b4/XXX0ffvn1x8OBBaDSaGpwSERE5g3ZROxQM+cDyfuXVAMzaMMKmXdjCYzBdvOjM0qgWqVJQefLJJ63e9+rVC2q1GuPHj8fZs2fRrFkzAICXlxdiYmIq3c/p06fx4Ycf4t1338WYMWMAAF26dEGLFi3w/vvvY9q0aVU9DyIiciaFAlkD3gJQz7IpyVeHpJFpNk0Hdk6AqbcCEMJ59VGtUeMxKvXr1wdw41bOvcrJyYHZbMbjjz9u2RYYGIi4uDhkZWXVtCQiIpKRdW034t0T2/HbO92g7BgJKBRSl0QupFpBxWQyoaSkBD/99BPmzZuHRx99FCEhIZbPjx49Co1GAw8PD3Tu3BlfffWV1fe1Wi0aNWqEgIAAq+2RkZHQarV3PHZpaSkMBoPVi4iI5EulUKK1uw+OD38fX2f9B5efqfyKO9HtqhVUWrZsCS8vL3Tu3BlNmzbFihUrLJ916tQJqampWLduHb788ks0aNAAQ4cOxapVqyxtdDod/P39bfYbEBCAwsLCOx47JSUFGo3G8goODq7OKRARkQTcFSp8MnMxLjzfXepSyEUohKj6TcMDBw6gqKgIBw8exPz58xEaGopvvvkGKpXKpq3ZbEb37t1hMBhw6NAhAMC4ceOwfft2m6snb7zxBl5++eU73kYqLS1FaWmp5b3BYEBwcDB6YTDcFO5VPRUiIqoOhQJvFfwXkR717t62AvnlRfixpAUAYO5PA9HmhdOWz8T1EpiLiuxSJsmXUZTjO6yDXq+Hn59fpe2qNJj2pqioKABAbGwsunTpgujoaKxduxbDhw+3aatUKvHYY49h2rRpuH79Ory8vBAQEGA1pfkmnU6HwMDAOx5brVZDrVZXp2wiIpKJMHdvhLlfBgCM6vkJTD+bLZ/1+DkRbh/dGP9Y72wJFDt/lqRGkodqBZU/i4qKgru7O44ePXrP34mIiMAff/wBnU5nNU5Fq9UiIiKipiUREZGLUSlujUTYFb0KeOfGf7+lC8E7Wx62fNZ0h4BP5m5nl0cSqvGsn927d6O8vByhoaEVfm42m5GZmYl27drBy8sLABAXFwelUonVq1db2ul0OuTk5GDAgAE1LYmIiGqJFwNO4PjjaZbXPxZ8hma7fNFsly+OL4yVujxygipdURk2bBgeeOABREVFwcvLCz///DMWLVqEqKgoDBkyBCdPnsRTTz2FkSNHok2bNtDpdHjvvfewZ88eq1ASFBSEZ555BlOnToVKpULz5s2xYMECaDQaTJgwwe4nSUREdiYEVhvux6wGd56paW+PehfjUe/tAIDiJ7eik+nvaLXKALHvoFPrIOepUlDp2rUrMjIysHDhQpjNZoSEhGDcuHF46aWX4OHhAV9fX2g0GsyfPx8XLlyAh4cHHnjgAWRnZyM+Pt5qX2+//TZ8fHwwffp0XL16FT169MCWLVu4Ki0RkYv475PRwCbnBpU/q6f0wJGn38OlvxVh4Iwp0Pxnl2S1kONUa9aPnBgMBmg0Gs76ISJyMmVUBLI3rZS6DADAthJgymvPInB5ntSl0D2611k/fHoyERG5vIc8gU9np+LpIyfh1qql1OWQHTGoEBFRtSivXsejvz2MnGJ5XM2O9KiHJF8dJuR8AxHbUepyyE4YVIiIqFqMBSdR2vM8Xp09Fvv/tBCn1B71LsZD7+9G/rtdoWrcSOpyqIYYVIiIqEY0n+/C3ye/gGJzGUzCfPcvOMGsBloUDPkA928+h/y0rlKXQzXAoEJERDXmuf4HPBbZF+2XPY/TxmtSl2Mxv9EvCA87I3UZVAMMKkREZBfmq1fRYs5O9Fs+DcXmyp/Z5mxxjQ7D1Ot+qcugamJQISIiu2o5eyc6L3tR6jIskgOP4/d4PiPOVdX4WT9ERES3a7lgDyIxCTufeQMBquo9Ybmqzhmv4azJw2b7/+WPQJs3jsDklCrI3hhUiIjI7kR5GVrM3YkYtynYPvoNNFJ5O/R4e0vLMO5fU9HwPdsF37xQwJDiwnjrh4iIHCbklTz0TJ/q8OMsOvtwhSGFXB+DChEROVTL1/cg8oNJKBeOu66hhACUKoftn6TDoEJERA4lysvQYt5udEh/HgsuhTtkrZXPQrbieArXS6mNGFSIiMjxzCa0fDUP30f7oOPuv9p99yqFEkLl0s/YpUowqBARkfOYTWgx4QLCt/9N6krIRTCoEBGRU5kuXkTrCSfR8YeRdhu3ojdfh1uxwi77InlhUCEiIqczXdGjyZDD6LpnlF32N+HkI2j5Kmf91EYMKkREJJmmfz2LyP/af8wK1R4uv+CbEDcGTxlRDnAcFRGRSzHqL8NrXWsYomo2E6i8qAxGUW6nqsgZjLjx/9fNv8croxB3ayFzx48fR+vWraUug4iIiKrh1KlTCAoKqvRzl7+iEhgYCAD4/fffodFoJK7GdRgMBgQHB+PUqVPw8/OTuhyXwD6rHvZb1bHPqof9VnVS9pkQAlevXkWzZs3u2M7lg4pSeWOYjUaj4Q9mNfj5+bHfqoh9Vj3st6pjn1UP+63qpOqze7nAwMG0REREJFsMKkRERCRbLh9U1Go1Zs+eDbVaLXUpLoX9VnXss+phv1Ud+6x62G9V5wp95vKzfoiIiKj2cvkrKkRERFR7MagQERGRbDGoEBERkWwxqBAREZFsuWxQ0Wq16N+/P7y9vdGkSRNMmzYNZWVlUpclmaNHj2LixImIjo6Gm5sb2rdvX2G79PR0hIWFwdPTEx07dsSGDRts2uj1eowdOxaBgYHw9fXF8OHDce7cOUefgtNlZmZi8ODBCAoKgre3N6Kjo/HRRx/ZPHeCfXZLVlYWevbsiYYNG0KtViM0NBTJycnQ6/VW7davX4+OHTvC09MTYWFhWL58uc2+ysrKMHXqVDRp0gTe3t7o378/jhw54qxTkcy1a9cQFBQEhUKBPXv2WH3Gn7VbPv74YygUCpvX9OnTrdqxzyr2ySefoFOnTvD09ESDBg2QkJCA69evWz53qd9R4YIKCwtF06ZNxUMPPSQ2bdok0tPThUajEc8995zUpUnmq6++EkFBQeKxxx4THTp0EO3atbNp88UXXwiFQiFmzZolvv32WzFhwgTh5uYm8vLyrNrFx8eLoKAgkZGRIdatWyfat28vOnbsKMrLy511Ok4RExMjkpKSxMqVK8XWrVvF9OnThVKpFHPmzLG0YZ9Z++yzz8TUqVPFqlWrRG5urliyZImoX7++6N+/v6XN9u3bhUqlEhMmTBDffvutmDVrllAoFCIzM9NqXxMmTBAajUakp6eLTZs2ib/85S+iefPm4sqVK84+LaeaNm2aaNy4sQAgfvzxR8t2/qxZW758uQAgNm3aJPLy8iyv33//3dKGfVax+fPnC19fX5GSkiK+++47sWrVKvHss8+Kq1evCiFc73fUJYPKggULhLe3t7h8+bJl2/vvvy9UKpU4c+aMhJVJx2QyWf77qaeeqjCohIWFiZEjR1pti42NFQkJCZb3O3fuFADE5s2bLdu0Wq1QKBQiIyPDAZVL5+LFizbbxo0bJ/z8/Cz9yT67uw8++EAAsPzuxcXFie7du1u1GTlypIiMjLS8P3XqlFCpVOL999+3bLt8+bLw9vYW//znP51TuAQOHz4svL29RVpamk1Q4c+atZtBpaLf05vYZ7a0Wq1wc3MTWVlZlbZxtd9Rl7z1k52djX79+lkeSAgAiYmJMJvNyMnJkbAy6dx85lFljh8/jvz8fCQmJlptT0pKwtatW1FaWgrgRt/6+/ujf//+ljbh4eGIjo5GVlaW/QuXUIMGDWy2derUCQaDAUVFReyze1S/fn0ANy4Tl5aWIjc3F48//rhVm6SkJBw+fBgnTpwAAOTk5MBsNlu1CwwMRFxcXK3usxdeeAETJ05EeHi41Xb+rFUd+6xiy5cvR6tWrZCQkFDh5674O+qSQUWr1SIiIsJqm7+/P5o2bQqtVitRVfJ2s19u77fIyEiUlZWhoKDA0i48PBwKhcKmXV3o2x07dqB58+bw9fVln92ByWRCSUkJfvrpJ8ybNw+PPvooQkJCcOzYMZSXl1fYZ8Ctn0OtVotGjRohICDApl1t7bNVq1bhl19+wauvvmrzGX/WKteuXTuoVCqEhoYiJSUFJpMJAPusMrt27UKHDh0wf/58NGrUCB4eHujRowd2794NAC75O+qST0/W6XTw9/e32R4QEIDCwkLnF+QCdDodANj0280fwpv9Vpf7dseOHVi5ciVSU1MBsM/upGXLljhz5gwA4OGHH8aKFSsAsM8qU1xcjOTkZCxYsKDCJ9Sy32w1bdoUc+fORbdu3aBQKPD1119j1qxZOHPmDJYuXco+q8T58+exd+9e/PLLL3j33XdRr149LFiwAHFxcfjtt99cst9cMqgQ2dvp06cxYsQI9O7dG5MnT5a6HNnLyspCUVERDh48iPnz52PQoEH45ptvpC5LtubPn4/GjRvj6aeflroUlxEfH4/4+HjL+7i4OHh5eeHNN9/EzJkzJaxM3sxmM65du4ZVq1YhKioKABATE4OQkBAsXbrUqk9dhUve+gkICLCZDgncSIB/HrdCt9xMy7f32810fbPf6mLfXrlyBQkJCahfvz5Wr15tGe/DPqtcVFQUYmNj8cwzz2DdunXIzc3F2rVr2WcVOHnyJFJTUzF37lzo9XpcuXIF165dA3BjqvK1a9fYb/coMTERJpMJ+/fvZ59VIiAgAPXr17eEFOBGX3Tq1AkHDx50yX5zyaASERFhc49Mr9fj3LlzNvfd6Iab/XJ7v2m1Wnh4eCA0NNTS7siRIzZriVQ0Lqg2uH79OgYOHAi9Xo/s7GxoNBrLZ+yzexMVFQV3d3ccPXoUrVu3hru7e4V9Btzq04iICPzxxx+WPxz/3K629VlBQQHKysrwyCOPICAgAAEBARg0aBAAoHfv3ujXrx9/1qqBfVaxdu3aVfpZSUmJa/6OOnWOkZ0sWLBA+Pj4CJ1OZ9m2bNmyOj09+c/uND151KhRVtt69OhR4VS+b775xrLtyJEjtXIqX3l5uRg4cKAIDAwUBw8erLAN++zu8vLyBADLucbFxYkHH3zQqs2oUaMqnPq4bNkyy7bCwkLh4+NT66Yn63Q6kZuba/V68803BQCRlpYm9u7dK4Tgz9q9SE5OFiqVSpw7d04IwT6ryOrVqwUAsW/fPsu2S5cuCR8fH/HKK68IIVzvd9Qlg8rNBd969uwpNm/eLD766CPh7+9fpxd8KyoqEpmZmSIzM1P06tVLBAcHW95fuHBBCCHEihUrhEKhEK+++qrIzc0VEydOFG5ubmLnzp1W+4qPjxfBwcHiyy+/FF9//bXo0KFDrVwcady4cQKASE1NtVpQKi8vT5SUlAgh2Ge3Gzp0qHj99dfF+vXrxZYtW0Rqaqpo0qSJiIqKEqWlpUKIW4tJPfvssyI3N1e8+uqrQqFQiC+//NJqXxMmTBD+/v7io48+Eps3bxY9e/asEwu+CSFEbm6uzToq/FmzFhcXJxYuXCg2btwoNm7cKCZMmCAUCoV48cUXLW3YZ7ZMJpPo0qWLaN26tVi5cqVYt26diImJEfXr17cEPFf7HXXJoCKEEIcOHRJ9+/YVXl5eolGjRuKll16y/EFZFxUUFAgAFb5yc3Mt7T788EPRpk0b4eHhITp06CDWr19vs68rV66IMWPGCH9/f+Hj4yOGDRtWK69UtWzZstI+KygosLRjn92SkpIioqOjha+vr/D29hbt2rUTr7zyitDr9Vbt1q1bJzp06CA8PDxEmzZtRHp6us2+SkpKxJQpU0SjRo2El5eX6Nevnzh8+LCzTkVSFQUVIfiz9meTJ08Wbdu2FV5eXkKtVosOHTqIt99+W5jNZqt27DNbFy9eFE8++aTQaDTCy8tLxMXF2Vw1dqXfUYUQt924IyIiIpIJlxxMS0RERHUDgwoRERHJFoMKERERyRaDChEREckWgwoRERHJFoMKERERyRaDChEREckWgwoRERHJFoMKERERyRaDChEREckWgwoRERHJ1v8DTFrsUvOW++kAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(binary_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "farm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
